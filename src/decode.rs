use std::{
    collections::VecDeque,
    ffi::CString,
    path::{Path, PathBuf},
    ptr::{null, null_mut},
    sync::{Arc, atomic::AtomicBool},
};

use derive_builder::Builder;
use ffmpeg_the_third::{
    ChannelLayout, Packet, Rational, Stream,
    ffi::{
        AV_CHANNEL_LAYOUT_STEREO, AVPixelFormat, AVSEEK_FLAG_BACKWARD, SwrContext,
        av_frame_get_buffer, av_hwdevice_ctx_create, av_hwframe_transfer_data,
        av_image_copy_to_buffer, av_image_get_buffer_size, avcodec_get_hw_config,
        avfilter_get_by_name, avfilter_graph_create_filter, avfilter_link, swr_alloc_set_opts2,
        swr_convert_frame, swr_free, swr_init,
    },
    filter::Graph,
    format::{Pixel, sample::Type, stream::Disposition},
    frame::{Audio, Video},
    software::scaling,
};

use time::format_description;
use tokio::{
    io::AsyncWriteExt,
    runtime::Handle,
    sync::{Notify, RwLock},
    task::JoinHandle,
};
use tracing::{Instrument, Level, info, span, warn};

use crate::{CURRENT_EXE_PATH, PlayerError, PlayerResult};
/// this wrapper type should be protected manually to
/// keep memory safe in multi threads
/// means need to wrap an Arc and a Lock to use it in multi threads
pub struct ManualProtectedInput(ffmpeg_the_third::format::context::Input);

unsafe impl Sync for ManualProtectedInput {}
/// this wrapper type should be protected manually to
/// keep memory safe in multi threads
/// means need to wrap an Arc and a Lock to use it in multi threads
pub struct ManualProtectedVideoDecoder(ffmpeg_the_third::decoder::Video);

unsafe impl Sync for ManualProtectedVideoDecoder {}
/// this wrapper type should be protected manually to
/// keep memory safe in multi threads
/// means need to wrap an Arc and a Lock to use it in multi threads
pub struct ManualProtectedAudioDecoder(ffmpeg_the_third::decoder::Audio);

unsafe impl Sync for ManualProtectedAudioDecoder {}
/// this wrapper type should be protected manually to
/// keep memory safe in multi threads
/// means need to wrap an Arc and a Lock to use it in multi threads
pub struct ManualProtectedResampler(pub *mut SwrContext);
unsafe impl Send for ManualProtectedResampler {}
unsafe impl Sync for ManualProtectedResampler {}
/// this wrapper type should be protected manually to
/// keep memory safe in multi threads
/// means need to wrap an Arc and a Lock to use it in multi threads
pub struct ManualProtectedConverter(pub scaling::Context);
unsafe impl Send for ManualProtectedConverter {}
unsafe impl Sync for ManualProtectedConverter {}
/// indicate which stream in the input is chose as main stream
#[derive(Debug, Clone, Copy)]
pub enum MainStream {
    Video,
    Audio,
}

/// represent all the details and relevent variables about
/// video format, decode, detail and hardware accelerate
/// the main struct of decode module to manage input and decode
pub struct TinyDecoder {
    video_stream_index: usize,
    audio_stream_index: usize,
    cover_stream_index: usize,
    main_stream: MainStream,
    video_time_base: Rational,
    audio_time_base: Rational,
    video_frame_rect: [u32; 2],
    format_duration: i64,
    end_timestamp: i64,
    end_time_formatted_string: String,
    format_input: Arc<RwLock<Option<ManualProtectedInput>>>,
    video_decoder: Arc<RwLock<Option<ManualProtectedVideoDecoder>>>,
    audio_decoder: Arc<RwLock<Option<ManualProtectedAudioDecoder>>>,
    converter_ctx: Option<ManualProtectedConverter>,
    resampler_ctx: Option<ManualProtectedResampler>,
    video_frame_cache_queue: Arc<RwLock<VecDeque<ffmpeg_the_third::frame::Video>>>,
    audio_frame_cache_queue: Arc<RwLock<VecDeque<ffmpeg_the_third::frame::Audio>>>,
    audio_packet_cache_queue: Arc<RwLock<VecDeque<ffmpeg_the_third::packet::Packet>>>,
    video_packet_cache_queue: Arc<RwLock<VecDeque<ffmpeg_the_third::packet::Packet>>>,
    demux_exit_flag: Arc<AtomicBool>,
    decode_exit_flag: Arc<AtomicBool>,
    demux_task_handle: Option<JoinHandle<()>>,
    decode_task_handle: Option<JoinHandle<()>>,
    hardware_config_flag: Arc<AtomicBool>,
    cover_pic_data: Arc<RwLock<Option<Vec<u8>>>>,
    runtime_handle: Handle,
    demux_thread_notify: Arc<Notify>,
    decode_thread_notify: Arc<Notify>,
}
impl TinyDecoder {
    /// init Decoder and new Struct
    /// `runtime_handle` is the handle of the tokio runtime in async_context
    pub fn new(runtime_handle: Handle) -> PlayerResult<Self> {
        ffmpeg_the_third::init().map_err(|e| PlayerError::Internal(e.to_string()))?;
        Ok(Self {
            video_stream_index: usize::MAX,
            audio_stream_index: usize::MAX,
            cover_stream_index: usize::MAX,
            main_stream: MainStream::Audio,
            video_time_base: Rational::new(1, 1),
            audio_time_base: Rational::new(1, 1),
            video_frame_rect: [0, 0],
            format_duration: 0,
            end_timestamp: 0,
            end_time_formatted_string: String::new(),
            format_input: Arc::new(RwLock::new(None)),
            video_decoder: Arc::new(RwLock::new(None)),
            audio_decoder: Arc::new(RwLock::new(None)),
            converter_ctx: None,
            resampler_ctx: None,
            video_frame_cache_queue: std::sync::Arc::new(RwLock::new(VecDeque::new())),
            audio_frame_cache_queue: std::sync::Arc::new(RwLock::new(VecDeque::new())),
            audio_packet_cache_queue: std::sync::Arc::new(RwLock::new(VecDeque::new())),
            video_packet_cache_queue: std::sync::Arc::new(RwLock::new(VecDeque::new())),
            demux_exit_flag: Arc::new(AtomicBool::new(false)),
            decode_exit_flag: Arc::new(AtomicBool::new(false)),
            demux_task_handle: None,
            decode_task_handle: None,
            hardware_config_flag: Arc::new(AtomicBool::new(false)),
            cover_pic_data: Arc::new(RwLock::new(None)),
            runtime_handle,
            demux_thread_notify: Arc::new(Notify::new()),
            decode_thread_notify: Arc::new(Notify::new()),
        })
    }
    /// reset all fields to the initial state
    /// this is to make the decoder ready for fresh input
    async fn reset_tiny_decoder_states(&mut self) {
        self.audio_stream_index = usize::MAX;
        self.video_stream_index = usize::MAX;
        self.cover_stream_index = usize::MAX;
        self.main_stream = MainStream::Audio;
        *self.audio_decoder.write().await = None;
        self.audio_time_base = Rational::new(1, 1);
        self.converter_ctx = None;
        self.cover_pic_data = Arc::new(RwLock::new(None));
        self.decode_task_handle = None;
        self.decode_exit_flag
            .store(false, std::sync::atomic::Ordering::Relaxed);
        self.demux_task_handle = None;
        self.demux_exit_flag
            .store(false, std::sync::atomic::Ordering::Relaxed);
        self.end_time_formatted_string = String::new();
        self.end_timestamp = 0;
        self.format_duration = 0;
        *self.format_input.write().await = None;
        self.hardware_config_flag
            .store(false, std::sync::atomic::Ordering::Relaxed);
        self.resampler_ctx = None;
        *self.video_decoder.write().await = None;
        self.video_frame_rect = [0, 0];
        self.video_time_base = Rational::new(1, 1);
        self.audio_packet_cache_queue.write().await.clear();
        self.video_packet_cache_queue.write().await.clear();
        self.audio_frame_cache_queue.write().await.clear();
        self.video_frame_cache_queue.write().await.clear();
    }
    /// called when user selected a file path to play
    /// init all the details from the file selected
    pub async fn set_file_path_and_init_par(&mut self, path: &Path) -> PlayerResult<()> {
        info!("ffmpeg version{}", ffmpeg_the_third::format::version());
        if self.demux_task_handle.is_some() {
            self.stop_demux_and_decode().await;
            self.reset_tiny_decoder_states().await;
        }
        let format_input = ffmpeg_the_third::format::input(path)
            .map_err(|e| PlayerError::Internal(e.to_string()))?;
        info!("input construct finished");
        let mut cover_stream = None;
        let mut video_stream = None;
        let mut audio_stream = None;
        for item in format_input.streams() {
            let stream_type = item.parameters().medium();
            if stream_type == ffmpeg_the_third::util::media::Type::Video {
                if item.disposition() == Disposition::ATTACHED_PIC {
                    info!("pic stream was found");
                    cover_stream = Some(item);
                } else {
                    info!("video stream was found");
                    video_stream = Some(item);
                }
            } else if stream_type == ffmpeg_the_third::util::media::Type::Audio {
                info!("audio stream was found");
                audio_stream = Some(item);
            } else if stream_type == ffmpeg_the_third::util::media::Type::Attachment {
                info!("attachment stream was found");
                cover_stream = Some(item);
            }
        }
        if audio_stream.is_none() && video_stream.is_none() {
            info!("no valid stream found");
        }
        if let Some(stream) = &cover_stream {
            info!("cover stream found");
            self.cover_stream_index = stream.index();
        }

        if let Some(stream) = &audio_stream {
            self.audio_stream_index = stream.index();
            self.audio_time_base = stream.time_base();
            info!("audio time_base==={}", self.audio_time_base);
        }

        if let Some(stream) = &video_stream {
            self.video_stream_index = stream.index();
            self.video_time_base = stream.time_base();
            info!("video time_base==={}", self.video_time_base);
            if audio_stream.is_none() {
                self.main_stream = MainStream::Video;
            }
        }

        // format_input.duration() can get the precise duration of the media file
        // format_input.duration() number unit is us
        info!("total duration {} us", format_input.duration());
        self.format_duration = format_input.duration();
        let adur_ts = {
            if let MainStream::Audio = self.main_stream {
                format_input.duration() * self.audio_time_base.denominator() as i64
                    / self.audio_time_base.numerator() as i64
                    / 1_000_000
            } else {
                format_input.duration() * self.video_time_base.denominator() as i64
                    / self.video_time_base.numerator() as i64
                    / 1_000_000
            }
        };
        self.end_timestamp = adur_ts;
        self.compute_and_set_end_time_str(adur_ts);

        if let Some(video_stream) = &video_stream {
            let video_decoder = self
                .choose_decoder_with_hardware_prefer(video_stream)
                .await?;

            let converter = ffmpeg_the_third::software::converter(
                (video_decoder.width(), video_decoder.height()),
                Pixel::YUV420P,
                Pixel::RGBA,
            )
            .map_err(|e| PlayerError::Internal(e.to_string()))?;
            self.converter_ctx = Some(ManualProtectedConverter(converter));

            info!("video decode format{:#?}", video_decoder.format());
            self.video_frame_rect = [video_decoder.width(), video_decoder.height()];
            let mut v_decoder = self.video_decoder.write().await;
            *v_decoder = Some(ManualProtectedVideoDecoder(video_decoder));
        }
        if let Some(audio_stream) = &audio_stream {
            let audio_decoder_ctx =
                ffmpeg_the_third::codec::Context::from_parameters(audio_stream.parameters())
                    .map_err(|e| PlayerError::Internal(e.to_string()))?;

            let mut audio_decoder = audio_decoder_ctx
                .decoder()
                .audio()
                .map_err(|e| PlayerError::Internal(e.to_string()))?;
            unsafe {
                if audio_decoder.ch_layout().channels() == 2 {
                    audio_decoder.set_ch_layout(ChannelLayout::STEREO);
                }
                let mut swr_ctx = null_mut();
                let r = swr_alloc_set_opts2(
                    &mut swr_ctx,
                    &AV_CHANNEL_LAYOUT_STEREO,
                    ffmpeg_the_third::ffi::AVSampleFormat::AV_SAMPLE_FMT_FLT,
                    48000,
                    audio_decoder.ch_layout().as_ptr(),
                    audio_decoder.format().into(),
                    audio_decoder.rate() as i32,
                    0,
                    null_mut(),
                );
                if r < 0 {
                    info!("swr ctx create err");
                }
                let r = swr_init(swr_ctx);
                if r < 0 {
                    info!("swr init err");
                }
                self.resampler_ctx = Some(ManualProtectedResampler(swr_ctx));
            }

            let mut a_decoder = self.audio_decoder.write().await;
            *a_decoder = Some(ManualProtectedAudioDecoder(audio_decoder));
        }
        {
            let mut input = self.format_input.write().await;
            *input = Some(ManualProtectedInput(format_input));
        }
        info!("par init finished!!!");
        self.start_process_input().await;
        Ok(())
    }
    /// the loop of demuxing video file
    async fn packet_demux_process(demux_context: DemuxContext) {
        info!("enter demux");
        loop {
            if demux_context
                .demux_exit_flag
                .load(std::sync::atomic::Ordering::Relaxed)
            {
                break;
            }
            /*
            choose to lock the packet vec first stick this in other functions
             */

            {
                let mut audio_packet_vec = demux_context.audio_packet_cache_queue.write().await;
                let mut video_packet_vec = demux_context.video_packet_cache_queue.write().await;
                let audio_stream_idx = demux_context.audio_stream_index;
                let video_stream_idx = demux_context.video_stream_index;
                let cover_index = demux_context.cover_stream_index;
                let mut cover_pic_data = demux_context.cover_image_data.write().await;
                let mut input = demux_context.format_input.write().await;
                if let Some(input) = &mut *input {
                    match input.0.packets().next() {
                        Some(Ok((stream, packet))) => {
                            if stream.index() == cover_index {
                                if let Some(d) = packet.data() {
                                    *cover_pic_data = Some(d.to_vec());
                                }
                            } else if stream.index() == audio_stream_idx {
                                audio_packet_vec.push_back(packet);
                            } else if stream.index() == video_stream_idx {
                                video_packet_vec.push_back(packet);
                            }
                        }
                        Some(Err(ffmpeg_the_third::util::error::Error::Eof)) => {
                            info!("demux process hit the end");
                        }
                        None => {}
                        _ => {}
                    }
                }
            }
            demux_context.demux_thread_notify.notified().await;
        }
    }
    ///convert the hardware output frame to middle format YUV420P
    async fn convert_hardware_frame(
        hardware_config: Arc<AtomicBool>,
        hardware_frame_converter: Arc<RwLock<Option<ManualProtectedConverter>>>,
        video_frame_tmp: Video,
    ) -> Video {
        if hardware_config.load(std::sync::atomic::Ordering::Relaxed) {
            unsafe {
                let mut transfered_frame = ffmpeg_the_third::frame::Video::empty();
                if 0 != av_hwframe_transfer_data(
                    transfered_frame.as_mut_ptr(),
                    video_frame_tmp.as_ptr(),
                    0,
                ) {
                    warn!("hardware frame transfer to software frame err");
                }

                transfered_frame.set_pts(video_frame_tmp.pts());
                let mut default_frame = Video::empty();
                {
                    let mut hardware_frame_converter_guard = hardware_frame_converter.write().await;
                    if let Some(hardware_frame_converter) = &mut *hardware_frame_converter_guard {
                        if hardware_frame_converter
                            .0
                            .run(&transfered_frame, &mut default_frame)
                            .is_ok()
                        {
                            default_frame.set_pts(transfered_frame.pts());
                            return default_frame;
                        }
                    } else if let Ok(mut ctx) = ffmpeg_the_third::software::converter(
                        (video_frame_tmp.width(), video_frame_tmp.height()),
                        transfered_frame.format(),
                        Pixel::YUV420P,
                    ) {
                        info!("transfered_frame format: {:?}", transfered_frame.format());
                        if ctx.run(&transfered_frame, &mut default_frame).is_ok() {
                            default_frame.set_pts(transfered_frame.pts());
                            *hardware_frame_converter_guard = Some(ManualProtectedConverter(ctx));
                            return default_frame;
                        }
                    }
                }
            }
        }
        video_frame_tmp
    }
    /// the loop of decoding demuxed packet
    async fn frame_decode_process(decode_context: DecodeContext) {
        info!("enter decode");
        let mut p = PathBuf::new();
        let mut graph = Graph::new();
        if decode_context.video_decoder.read().await.is_some() {
            if let Ok(exe_path) = CURRENT_EXE_PATH.as_ref() {
                if let Some(exe_folder) = exe_path.parent() {
                    p = exe_folder.join("app_font.ttf");
                    if tokio::fs::File::open(&p).await.is_err() {
                        if let Ok(mut file) = tokio::fs::File::create_new(&p).await {
                            if file.write_all(crate::appui::MAPLE_FONT).await.is_ok() {}
                        }
                    }
                }
            }

            if let Some(font_path_str) = p.to_str() {
                let mut font_path_str = font_path_str.replace("\\", "/");
                if let Some(idx) = font_path_str.find(':') {
                    font_path_str.insert(idx, '\\');
                    unsafe {
                        if let Ok(c_str_buffer) = CString::new("buffer") {
                            if let Ok(c_str_buffersrc) = CString::new("buffersrc") {
                                if let Ok(c_str_buffersrc_args) = CString::new(format!(
                                    "video_size={}x{}:pix_fmt={}:time_base={}/{}:pixel_aspect=1/1",
                                    decode_context.video_frame_rect[0],
                                    decode_context.video_frame_rect[1],
                                    AVPixelFormat::AV_PIX_FMT_YUV420P as i32,
                                    decode_context.video_time_base.numerator(),
                                    decode_context.video_time_base.denominator(),
                                )) {
                                    if let Ok(c_str_drawtext) = CString::new("drawtext") {
                                        if let Ok(c_str_draw) = CString::new("draw") {
                                            if let Ok(c_str_draw_args) = CString::new(format!(
                                                "text='Tiny Player':fontfile={}:fontsize=26:fontcolor=white@0.3:x=w-text_w-10:y=10",
                                                font_path_str
                                            )) {
                                                if let Ok(c_str_buffersink) =
                                                    CString::new("buffersink")
                                                {
                                                    if let Ok(c_str_sink) = CString::new("sink") {
                                                        let buffersrc_filter = avfilter_get_by_name(
                                                            c_str_buffer.as_ptr(),
                                                        );
                                                        // graph free will automatically free filterctx
                                                        let mut buffersrc_ctx = null_mut();
                                                        let draw_filter = avfilter_get_by_name(
                                                            c_str_drawtext.as_ptr(),
                                                        );
                                                        let mut drawtext_ctx = null_mut();
                                                        let buffersink_filter =
                                                            avfilter_get_by_name(
                                                                c_str_buffersink.as_ptr(),
                                                            );
                                                        let mut buffersink_ctx = null_mut();
                                                        let r = avfilter_graph_create_filter(
                                                            &mut buffersrc_ctx,
                                                            buffersrc_filter,
                                                            c_str_buffersrc.as_ptr(),
                                                            c_str_buffersrc_args.as_ptr(),
                                                            null_mut(),
                                                            graph.as_mut_ptr(),
                                                        );
                                                        if r < 0 {
                                                            info!("create buffer filter err");
                                                        }
                                                        let r = avfilter_graph_create_filter(
                                                            &mut drawtext_ctx,
                                                            draw_filter,
                                                            c_str_draw.as_ptr(),
                                                            c_str_draw_args.as_ptr(),
                                                            null_mut(),
                                                            graph.as_mut_ptr(),
                                                        );
                                                        if r < 0 {
                                                            info!("create drawtext filter err");
                                                        }
                                                        let r = avfilter_graph_create_filter(
                                                            &mut buffersink_ctx,
                                                            buffersink_filter,
                                                            c_str_sink.as_ptr(),
                                                            null(),
                                                            null_mut(),
                                                            graph.as_mut_ptr(),
                                                        );
                                                        if r < 0 {
                                                            info!("create buffersink filter err");
                                                        }
                                                        let r = avfilter_link(
                                                            buffersrc_ctx,
                                                            0,
                                                            drawtext_ctx,
                                                            0,
                                                        );
                                                        if r < 0 {
                                                            info!("link src and drawtext err");
                                                        }
                                                        let r = avfilter_link(
                                                            drawtext_ctx,
                                                            0,
                                                            buffersink_ctx,
                                                            0,
                                                        );
                                                        if r < 0 {
                                                            info!("link drawtext and sink err");
                                                        }
                                                        if graph.validate().is_ok() {
                                                            info!(
                                                                "graph validate success!dump:\n{}",
                                                                graph.dump()
                                                            );
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        let hardware_frame_converter = Arc::new(RwLock::new(None));
        loop {
            if decode_context
                .decode_exit_flag
                .load(std::sync::atomic::Ordering::Relaxed)
            {
                break;
            }
            if !decode_context
                .audio_packet_cache_queue
                .read()
                .await
                .is_empty()
            {
                let mut audio_packet_cache_vec =
                    decode_context.audio_packet_cache_queue.write().await;
                let mut a_frame_vec = decode_context.audio_frame_cache_queue.write().await;
                let mut audio_decoder = decode_context.audio_decoder.write().await;
                if audio_packet_cache_vec.len() < 10 {
                    decode_context.demux_thread_notify.notify_one();
                }
                if let Some(front_packet) = audio_packet_cache_vec.pop_front() {
                    if let Some(decoder) = &mut *audio_decoder {
                        if decoder.0.send_packet(&front_packet).is_ok() {
                            loop {
                                let mut audio_frame_tmp = ffmpeg_the_third::frame::Audio::empty();

                                if decoder.0.receive_frame(&mut audio_frame_tmp).is_err() {
                                    break;
                                }

                                a_frame_vec.push_back(audio_frame_tmp);
                            }
                        }
                    }
                }
            } else {
                decode_context.demux_thread_notify.notify_one();
            }
            if !decode_context
                .video_packet_cache_queue
                .read()
                .await
                .is_empty()
            {
                let mut video_packet_cache_vec =
                    decode_context.video_packet_cache_queue.write().await;
                let mut v_frame_vec = decode_context.video_frame_cache_queue.write().await;
                let mut v_decoder = decode_context.video_decoder.write().await;
                // info!("video frame vec len{}", frames.len());
                if video_packet_cache_vec.len() < 10 {
                    decode_context.demux_thread_notify.notify_one();
                }
                if let Some(front_packet) = video_packet_cache_vec.pop_front() {
                    if let Some(decoder) = &mut *v_decoder {
                        if decoder.0.send_packet(&front_packet).is_ok() {
                            loop {
                                let mut video_frame_tmp = ffmpeg_the_third::frame::Video::empty();

                                if decoder.0.receive_frame(&mut video_frame_tmp).is_err() {
                                    break;
                                }

                                let video_frame = TinyDecoder::convert_hardware_frame(
                                    decode_context.hardware_config_flag.clone(),
                                    hardware_frame_converter.clone(),
                                    video_frame_tmp,
                                )
                                .await;

                                if let Some(mut ctx) = graph.get("buffersrc") {
                                    if ctx.source().add(&video_frame).is_ok() {
                                        let mut filtered_frame =
                                            ffmpeg_the_third::frame::Video::empty();
                                        if let Some(mut ctx) = graph.get("sink") {
                                            if ctx.sink().frame(&mut filtered_frame).is_ok() {
                                                v_frame_vec.push_back(filtered_frame);
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            } else {
                decode_context.demux_thread_notify.notify_one();
            }
            decode_context.decode_thread_notify.notified().await;
        }
    }
    /// start the demux and decode task
    async fn start_process_input(&mut self) {
        if let Ok(demux_context) = DemuxContextBuilder::default()
            .audio_stream_index(self.audio_stream_index)
            .video_stream_index(self.video_stream_index)
            .format_input(self.format_input.clone())
            .audio_packet_cache_queue(self.audio_packet_cache_queue.clone())
            .video_packet_cache_queue(self.video_packet_cache_queue.clone())
            .cover_stream_index(self.cover_stream_index)
            .cover_image_data(self.cover_pic_data.clone())
            .demux_exit_flag(self.demux_exit_flag.clone())
            .demux_thread_notify(self.demux_thread_notify.clone())
            .build()
        {
            self.demux_task_handle = Some(self.runtime_handle.spawn(async move {
                let demux_span = span!(Level::INFO, "demux");
                let _demux_entered = demux_span.enter();
                Self::packet_demux_process(demux_context)
                    .in_current_span()
                    .await;
            }));
        } else {
            warn!("build demux context error!");
        }

        if let Ok(decode_context) = DecodeContextBuilder::default()
            .video_decoder(self.video_decoder.clone())
            .audio_decoder(self.audio_decoder.clone())
            .video_frame_cache_queue(self.video_frame_cache_queue.clone())
            .audio_frame_cache_queue(self.audio_frame_cache_queue.clone())
            .audio_packet_cache_queue(self.audio_packet_cache_queue.clone())
            .video_packet_cache_queue(self.video_packet_cache_queue.clone())
            .hardware_config_flag(self.hardware_config_flag.clone())
            .decode_exit_flag(self.decode_exit_flag.clone())
            .video_time_base(self.video_time_base)
            .video_frame_rect(self.video_frame_rect)
            .decode_thread_notify(self.decode_thread_notify.clone())
            .demux_thread_notify(self.demux_thread_notify.clone())
            .build()
        {
            self.decode_task_handle = Some(self.runtime_handle.spawn(async move {
                let demux_span = span!(Level::INFO, "decode");
                let _demux_entered = demux_span.enter();
                Self::frame_decode_process(decode_context)
                    .in_current_span()
                    .await;
            }));
        } else {
            warn!("build decode context error!");
        }
    }
    /// called by the main thread pull one audio frame from the queue
    /// in addition, do the resample
    pub async fn pull_one_audio_play_frame(&mut self) -> Option<ffmpeg_the_third::frame::Audio> {
        if let Some(resampler_ctx) = &mut self.resampler_ctx {
            let mut res = ffmpeg_the_third::frame::Audio::empty();
            res.set_format(ffmpeg_the_third::format::Sample::F32(Type::Packed));
            res.set_ch_layout(ChannelLayout::STEREO);
            res.set_rate(48000);

            {
                let mut a_frame_vec = self.audio_frame_cache_queue.write().await;
                if a_frame_vec.len() < 10 {
                    self.decode_thread_notify.notify_one();
                }
                if !a_frame_vec.is_empty() {
                    if let Some(raw_frame) = a_frame_vec.pop_front() {
                        unsafe {
                            let r = swr_convert_frame(
                                resampler_ctx.0,
                                res.as_mut_ptr(),
                                raw_frame.as_ptr(),
                            );
                            if r == 0 {
                                if let Some(pts) = raw_frame.pts() {
                                    res.set_pts(Some(pts));
                                    res.set_rate(48000);
                                    return Some(res);
                                }
                            } else {
                                info!("resample err{}", r);
                            }
                        }
                    }
                }
            }
        }

        None
    }
    pub async fn _convert_frame_data_to_no_padding_layout(res: &mut Video) -> Box<[u8]> {
        unsafe {
            let buf_size = av_image_get_buffer_size(
                AVPixelFormat::AV_PIX_FMT_RGBA,
                res.width() as i32,
                res.height() as i32,
                1,
            );
            let mut buf = vec![0_u8; buf_size as usize];
            let frame = res.as_mut_ptr();

            if 0 > av_image_copy_to_buffer(
                buf.as_mut_ptr(),
                buf_size,
                (*frame).data.as_ptr() as *const *const u8,
                (*frame).linesize.as_ptr(),
                AVPixelFormat::from(res.format()),
                (*frame).width,
                (*frame).height,
                1,
            ) {
                warn!("av_image_copy_to_buffer err");
            }
            buf.into_boxed_slice()
        }
    }

    /// pull one frame from the video cache queue
    /// in additon, do the convert and if the input changed(caused by source or hard acce)
    /// set the new converter, only change the out put format, dont change the width and height which
    /// have been used in the ui thread
    pub async fn pull_one_video_play_frame(&mut self) -> Option<ffmpeg_the_third::frame::Video> {
        if let Some(converter_ctx) = &mut self.converter_ctx {
            let mut res = ffmpeg_the_third::frame::Video::empty();

            let mut return_val = None;
            {
                let mut v_frame_vec = self.video_frame_cache_queue.write().await;
                if v_frame_vec.len() < 5 {
                    self.decode_thread_notify.notify_one();
                }
                if !v_frame_vec.is_empty() {
                    if let Some(raw_frame) = v_frame_vec.pop_front() {
                        // info!("raw frame pts{}", raw_frame.pts().unwrap());
                        unsafe {
                            let frame = res.as_mut_ptr();
                            (*frame).width = raw_frame.width() as i32;
                            (*frame).height = raw_frame.height() as i32;
                            (*frame).format = AVPixelFormat::AV_PIX_FMT_RGBA as i32;

                            // align to 256 to meet the wgpu requirement
                            let align = 256;
                            if 0 > av_frame_get_buffer(frame, align) {
                                warn!("av_frame_get_buffer error!");
                            }
                        }
                        if converter_ctx.0.run(&raw_frame, &mut res).is_ok() {
                            if let Some(pts) = raw_frame.pts() {
                                res.set_pts(Some(pts));
                            }

                            return_val = Some(res);
                        }
                    }
                }
            }
            return return_val;
        }
        None
    }
    /// get v time base used to check time and compare to sync
    pub fn video_time_base(&self) -> &Rational {
        &self.video_time_base
    }
    /// get a time base used to check time and compare to sync
    pub fn audio_time_base(&self) -> &Rational {
        &self.audio_time_base
    }
    /// get the calculated end time str
    pub fn end_time_formatted_string(&self) -> &String {
        &self.end_time_formatted_string
    }
    /// video_frame_rect to config the main colorimage and texture size
    pub fn video_frame_rect(&self) -> &[u32; 2] {
        &self.video_frame_rect
    }
    /// get the end audio timestamp used as the main time flow
    /// it is more accurate than just use time second
    pub fn end_ts(&self) -> i64 {
        self.end_timestamp
    }
    /// seek the input to a selected timestamp
    /// use the ffi function to enable seek all the frames
    /// the ffmpeg_the_third::ffi::AVSEEK_FLAG_ANY flag makes sure
    /// the seek would go as I want, to an exact frame
    pub fn seek_timestamp_to_decode(&self, ts: i64) {
        {
            let mut audio_packet_cache_vec = self
                .runtime_handle
                .block_on(self.audio_packet_cache_queue.write());
            let mut video_packet_cache_vec = self
                .runtime_handle
                .block_on(self.video_packet_cache_queue.write());
            let mut audio_cache_vec = self
                .runtime_handle
                .block_on(self.audio_frame_cache_queue.write());
            let mut video_cache_vec = self
                .runtime_handle
                .block_on(self.video_frame_cache_queue.write());
            audio_packet_cache_vec.clear();
            video_packet_cache_vec.clear();
            audio_cache_vec.clear();
            video_cache_vec.clear();

            let mut input = self.runtime_handle.block_on(self.format_input.write());
            let main_stream_idx = {
                if let MainStream::Audio = self.main_stream {
                    self.audio_stream_index
                } else {
                    self.video_stream_index
                }
            };
            let main_stream_time_base = {
                if let MainStream::Audio = self.main_stream {
                    &self.audio_time_base
                } else {
                    &self.video_time_base
                }
            };
            unsafe {
                info!("seek timestamp:{}", ts);
                if let Some(input) = &mut *input {
                    let res = ffmpeg_the_third::ffi::avformat_seek_file(
                        input.0.as_mut_ptr(),
                        main_stream_idx as i32,
                        ts - main_stream_time_base.denominator() as i64
                            / main_stream_time_base.numerator() as i64,
                        ts,
                        ts + main_stream_time_base.denominator() as i64
                            / main_stream_time_base.numerator() as i64,
                        AVSEEK_FLAG_BACKWARD,
                    );
                    if res != 0 {
                        info!("seek err num:{res}");
                    }
                }
            }
            self.runtime_handle.block_on(self.flush_decoders());
        }
    }
    /// use the file detail to compute the video duration and make str to inform the user
    fn compute_and_set_end_time_str(&mut self, end_ts: i64) {
        let sec_num = {
            if let MainStream::Audio = self.main_stream {
                end_ts * self.audio_time_base.numerator() as i64
                    / self.audio_time_base.denominator() as i64
            } else {
                end_ts * self.video_time_base.numerator() as i64
                    / self.video_time_base.denominator() as i64
            }
        };
        let sec = (sec_num % 60) as u8;
        let min_num = sec_num / 60;
        let min = (min_num % 60) as u8;
        let hour_num = min_num / 60;
        let hour = hour_num as u8;
        info!("hour{},min{},sec{}", hour, min, sec);
        if let Ok(time) = time::Time::from_hms(hour, min, sec) {
            if let Ok(formatter) = format_description::parse("[hour]:[minute]:[second]") {
                if let Ok(s) = time.format(&formatter) {
                    self.end_time_formatted_string = s;
                }
            }
        } else {
            info!("end_time_err");
        }
    }
    /// give an Arc of cover_pic_data out
    pub fn cover_pic_data(&self) -> Arc<RwLock<Option<Vec<u8>>>> {
        self.cover_pic_data.clone()
    }
    /// determin if the input is exist
    pub async fn is_input_exist(&self) -> bool {
        let input = self.format_input.write().await;
        input.is_some()
    }
    /// read the mainstream
    pub fn main_stream(&self) -> &MainStream {
        &self.main_stream
    }
    /// read video stream index
    pub fn _video_stream_idx(&self) -> usize {
        self.video_stream_index
    }
    /// stop demux and decode
    async fn stop_demux_and_decode(&mut self) {
        self.demux_exit_flag
            .store(true, std::sync::atomic::Ordering::Relaxed);
        self.demux_thread_notify.notify_one();
        if let Some(handle) = &mut self.demux_task_handle {
            if handle.await.is_ok() {
                info!("demux thread join success");
            }
        }
        self.decode_exit_flag
            .store(true, std::sync::atomic::Ordering::Relaxed);
        self.decode_thread_notify.notify_one();
        if let Some(handle) = &mut self.decode_task_handle {
            if handle.await.is_ok() {
                info!("decode thread join success");
            }
        }
    }
    /// flush decoder , be called after seek file is done
    async fn flush_decoders(&self) {
        let mut a_decoder = self.audio_decoder.write().await;
        if let Some(a) = &mut *a_decoder {
            a.0.flush();
        }
        let mut v_decoder = self.video_decoder.write().await;
        if let Some(v) = &mut *v_decoder {
            v.0.flush();
        }
    }
}

impl TinyDecoder {
    /// enable hardware accelerate for video decode, currently use d3d12 only on windows
    /// others like vulkan are in developing
    /// fallback to softerware decoder if doesnt support
    async fn choose_decoder_with_hardware_prefer(
        &mut self,
        stream: &Stream<'_>,
    ) -> PlayerResult<ffmpeg_the_third::decoder::Video> {
        let codec_ctx = ffmpeg_the_third::codec::Context::from_parameters(stream.parameters())
            .map_err(|e| PlayerError::Internal(e.to_string()))?;

        let mut decoder = codec_ctx
            .decoder()
            .video()
            .map_err(|e| PlayerError::Internal(e.to_string()))?;
        unsafe {
            if let Some(codec) = &decoder.codec() {
                let hw_config = avcodec_get_hw_config(codec.as_ptr(), 0);

                if hw_config.is_null() {
                    warn!("currently doesn't support hardware accelerate");
                    Ok(decoder)
                } else {
                    let mut hw_device_ctx = null_mut();
                    if 0 != av_hwdevice_ctx_create(
                        &mut hw_device_ctx,
                        (*hw_config).device_type,
                        null(),
                        null_mut(),
                        0,
                    ) {
                        warn!("hw device create err");
                        return Ok(decoder);
                    }
                    (*decoder.as_mut_ptr()).hw_device_ctx = hw_device_ctx;

                    self.hardware_config_flag
                        .store(true, std::sync::atomic::Ordering::Relaxed);
                    warn!("hardware decode acceleration is on!");
                    Ok(decoder)
                }
            } else {
                Err(PlayerError::Internal(
                    "err when config hardware acc".to_string(),
                ))
            }
        }
    }
}
impl Drop for TinyDecoder {
    /// handle some struct that have to be free manually
    fn drop(&mut self) {
        self.demux_exit_flag
            .store(true, std::sync::atomic::Ordering::Relaxed);
        self.decode_exit_flag
            .store(true, std::sync::atomic::Ordering::Relaxed);
        self.decode_thread_notify.notify_waiters();
        let demux_task_handle = self.demux_task_handle.take();
        let decode_task_handle = self.decode_task_handle.take();
        self.runtime_handle.spawn(async move {
            demux_task_handle
                .ok_or(PlayerError::Internal("join demux thread err".to_string()))?
                .await
                .map_err(|_e| PlayerError::Internal("join demux thread err".to_string()))?;
            decode_task_handle
                .ok_or(PlayerError::Internal("join decode thread err".to_string()))?
                .await
                .map_err(|_e| PlayerError::Internal("join decode thread err".to_string()))?;
            info!("demux and decode thread exit gracefully");
            PlayerResult::Ok(())
        });

        if let Some(ctx) = &mut self.resampler_ctx {
            unsafe {
                swr_free(&mut ctx.0);
            }
        }
    }
}
#[derive(Builder)]
struct DemuxContext {
    pub audio_stream_index: usize,
    pub video_stream_index: usize,
    pub cover_stream_index: usize,
    pub format_input: Arc<RwLock<Option<ManualProtectedInput>>>,
    pub audio_packet_cache_queue: Arc<RwLock<VecDeque<Packet>>>,
    pub video_packet_cache_queue: Arc<RwLock<VecDeque<Packet>>>,
    pub cover_image_data: Arc<RwLock<Option<Vec<u8>>>>,
    pub demux_exit_flag: Arc<AtomicBool>,
    pub demux_thread_notify: Arc<Notify>,
}

#[derive(Builder)]
struct DecodeContext {
    pub audio_decoder: Arc<RwLock<Option<ManualProtectedAudioDecoder>>>,
    pub video_decoder: Arc<RwLock<Option<ManualProtectedVideoDecoder>>>,
    pub audio_packet_cache_queue: Arc<RwLock<VecDeque<Packet>>>,
    pub video_packet_cache_queue: Arc<RwLock<VecDeque<Packet>>>,
    pub audio_frame_cache_queue: Arc<RwLock<VecDeque<Audio>>>,
    pub video_frame_cache_queue: Arc<RwLock<VecDeque<Video>>>,
    pub hardware_config_flag: Arc<AtomicBool>,
    pub decode_exit_flag: Arc<AtomicBool>,
    pub video_time_base: Rational,
    pub video_frame_rect: [u32; 2],
    pub demux_thread_notify: Arc<Notify>,
    pub decode_thread_notify: Arc<Notify>,
}
